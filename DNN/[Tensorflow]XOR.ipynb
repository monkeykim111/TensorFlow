{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Tensorflow]XOR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj-KQwtMQ1pG"
      },
      "source": [
        "#### XOR - Logistic Regression - Eager Excuetion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij4FVNfjQ-5t"
      },
      "source": [
        "기본 라이브러리 및 Tensorflow 버전 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFBBS1KmQllb",
        "outputId": "075cb052-adc7-4f62-af49-e906bce55253"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(777)\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO0pQ_uLRR5U"
      },
      "source": [
        "x_data가 2차원 배열이기 때문에 2차원 공간에 표현하여 x1과 x2를 기준으로 y_data 0과 1로 구분</br>\n",
        "붉은 색과 푸른 색으로 0과 1을 표시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "o9yvv5lJRHk-",
        "outputId": "fc71af58-5097-423b-a70d-2eca59acf303"
      },
      "source": [
        "x_data = [[0,0],\n",
        "          [0,1],\n",
        "          [1,0],\n",
        "          [1,1]]\n",
        "\n",
        "y_data = [[0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [0]]\n",
        "\n",
        "plt.scatter(x_data[0][0], x_data[0][1], c='red', marker='^')\n",
        "plt.scatter(x_data[3][0], x_data[3][1], c='red', marker='^')\n",
        "plt.scatter(x_data[1][0], x_data[1][1], c='blue', marker='^')\n",
        "plt.scatter(x_data[2][0], x_data[2][1], c='blue', marker='^')\n",
        "\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIEJDc_GS0YW"
      },
      "source": [
        "#### Tensorflow Eager\n",
        "\n",
        "- Tensorflow data API를 통해 학습시킬 값들을 담는다.(Batch size는 한 번에 학습시킬 size로 정한다.)\n",
        "- preprocess function으로 features, labels는 실제 학습에 쓰일 data 연산을 위해 type을 맞춰준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjqyEGPMStzS"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
        "\n",
        "def preprocess_data(feature, labels):\n",
        "  feature = tf.cast(feature, tf.float32)\n",
        "  labels = tf.cast(labels, tf.float32)\n",
        "  return feature, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myYs1PVJUJiu"
      },
      "source": [
        "W와 b는 학습을 통해 생성되는 모델에 쓰이는 Weight와 bias임</br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfBkgjVCTu1l",
        "outputId": "2cffdb8c-7114-486f-b2bc-52da7852ffc1"
      },
      "source": [
        "W = tf.Variable(tf.zeros((2,1)), name='weight')\n",
        "b = tf.Variable(tf.zeros((1,)), name='bias')\n",
        "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W = [[0.]\n",
            " [0.]], B = [0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZxyno-SUwQG"
      },
      "source": [
        "Sigmoid 함수를 가설로 선언\n",
        "- sigmoid는 0과 1값만을 리턴함\n",
        "-  tf.sigmoid(tf.matmul(X, W) + b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njz7y5XxUzfO"
      },
      "source": [
        "def logistic_regression(features):\n",
        "  hypothesis = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
        "  return hypothesis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMZfeStNVV-P"
      },
      "source": [
        "가설을 검증할 cast 함수를 정의함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EULcn4jtVTtg"
      },
      "source": [
        "def loss_fn(hypothesis, features, labels):\n",
        "  cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "  return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNfJFpbfVuml"
      },
      "source": [
        "추론한 값은 0.5를 기준으로 0과 1의 값을 리턴함\n",
        "- sigmoid 함수를 통해 예측한 값이 0.5보다 크면 1을, 작으면 0을 반환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy8qcf6IVsNV"
      },
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvtSYe0kV_XG"
      },
      "source": [
        "GradientTape을 통해 경사값을 계산함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RAC_nGHV-Dx"
      },
      "source": [
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
        "    return tape.gradient(loss_value, [W,b])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eI3K-jIWFGb",
        "outputId": "4509adcc-74ab-4a82-f72b-829aa48ddb03"
      },
      "source": [
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in dataset:\n",
        "        features, labels = preprocess_data(features, labels)\n",
        "        grads = grad(logistic_regression(features), features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
        "print(\"W = {}, B = {}\".format(W.numpy(), b.numpy()))\n",
        "x_data, y_data = preprocess_data(x_data, y_data)\n",
        "test_acc = accuracy_fn(logistic_regression(x_data),y_data)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.6931\n",
            "Iter: 100, Loss: 0.6931\n",
            "Iter: 200, Loss: 0.6931\n",
            "Iter: 300, Loss: 0.6931\n",
            "Iter: 400, Loss: 0.6931\n",
            "Iter: 500, Loss: 0.6931\n",
            "Iter: 600, Loss: 0.6931\n",
            "Iter: 700, Loss: 0.6931\n",
            "Iter: 800, Loss: 0.6931\n",
            "Iter: 900, Loss: 0.6931\n",
            "Iter: 1000, Loss: 0.6931\n",
            "W = [[0.]\n",
            " [0.]], B = [0.]\n",
            "Testset Accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2io51lQbWb1t"
      },
      "source": [
        "#### Neural Network를 통해 xor 해결하기\n",
        "- data를 3Layer의 Neural Network를 통해 학습시킨 후 모델을 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1d8nSUiWiIY"
      },
      "source": [
        "W1 = tf.Variable(tf.random.normal((2, 1)), name='weight1')\n",
        "b1 = tf.Variable(tf.random.normal((1,)), name='bias1')\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal((2, 1)), name='weight2')\n",
        "b2 = tf.Variable(tf.random.normal((1,)), name='bias2')\n",
        "\n",
        "W3 = tf.Variable(tf.random.normal((2, 1)), name='weight3')\n",
        "b3 = tf.Variable(tf.random.normal((1,)), name='bias3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTiYxXnHWxCn"
      },
      "source": [
        "def neural_net(features):\n",
        "    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n",
        "    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n",
        "    layer3 = tf.concat([layer1, layer2],-1)\n",
        "    layer3 = tf.reshape(layer3, shape = [-1,2])\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer3, W3) + b3)\n",
        "    return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, labels):\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "    return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "def accuracy_fn(hypothesis, labels):\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "def grad(hypothesis, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss_fn(neural_net(features),labels)\n",
        "    return tape.gradient(loss_value, [W1, W2, W3, b1, b2, b3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSNtFxJW26m",
        "outputId": "ae9e0cc2-d8aa-49af-bfa1-b1e776a90da5"
      },
      "source": [
        "\n",
        "EPOCHS = 50000\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in dataset:\n",
        "        features, labels = preprocess_data(features, labels)\n",
        "        grads = grad(neural_net(features), features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W1, W2, W3, b1, b2, b3]))\n",
        "        if step % 5000 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(neural_net(features),labels)))\n",
        "x_data, y_data = preprocess_data(x_data, y_data)\n",
        "test_acc = accuracy_fn(neural_net(x_data),y_data)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.8487\n",
            "Iter: 5000, Loss: 0.6847\n",
            "Iter: 10000, Loss: 0.6610\n",
            "Iter: 15000, Loss: 0.6154\n",
            "Iter: 20000, Loss: 0.5722\n",
            "Iter: 25000, Loss: 0.5433\n",
            "Iter: 30000, Loss: 0.5211\n",
            "Iter: 35000, Loss: 0.4911\n",
            "Iter: 40000, Loss: 0.4416\n",
            "Iter: 45000, Loss: 0.3313\n",
            "Testset Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxqEufZXYUH"
      },
      "source": [
        "#### Deep Neural Network로 XOR 해결하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jchmHdxCW38s"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n",
        "nb_classes = 10\n",
        "\n",
        "class wide_deep_nn():\n",
        "    def __init__(self, nb_classes):\n",
        "        super(wide_deep_nn, self).__init__()        \n",
        "     \n",
        "        self.W1 = tf.Variable(tf.random.normal((2, nb_classes)), name='weight1')\n",
        "        self.b1 = tf.Variable(tf.random.normal((nb_classes,)), name='bias1')\n",
        "\n",
        "        self.W2 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight2')\n",
        "        self.b2 = tf.Variable(tf.random.normal((nb_classes,)), name='bias2')\n",
        "\n",
        "        self.W3 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight3')\n",
        "        self.b3 = tf.Variable(tf.random.normal((nb_classes,)), name='bias3')\n",
        "\n",
        "        self.W4 = tf.Variable(tf.random.normal((nb_classes, 1)), name='weight4')\n",
        "        self.b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n",
        "        \n",
        "        self.variables = [self.W1,self.b1,self.W2,self.b2,self.W3,self.b3,self.W4,self.b4]\n",
        "        \n",
        "    def preprocess_data(self, features, labels):\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        return features, labels\n",
        "        \n",
        "    def deep_nn(self, features):\n",
        "        layer1 = tf.sigmoid(tf.matmul(features, self.W1) + self.b1)\n",
        "        layer2 = tf.sigmoid(tf.matmul(layer1, self.W2) + self.b2)\n",
        "        layer3 = tf.sigmoid(tf.matmul(layer2, self.W3) + self.b3)\n",
        "        hypothesis = tf.sigmoid(tf.matmul(layer3, self.W4) + self.b4)\n",
        "        return hypothesis\n",
        "    \n",
        "    def loss_fn(self, hypothesis, features, labels):\n",
        "        cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
        "        return cost\n",
        "\n",
        "    def accuracy_fn(self, hypothesis, labels):\n",
        "        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "        return accuracy\n",
        "\n",
        "    def grad(self, hypothesis, features, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_value = self.loss_fn(self.deep_nn(features),features,labels)\n",
        "        return tape.gradient(loss_value,self.variables)\n",
        "    \n",
        "    def fit(self, dataset, EPOCHS=20000, verbose=500):\n",
        "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "        for step in range(EPOCHS):\n",
        "            for features, labels  in dataset:\n",
        "                features, labels = self.preprocess_data(features, labels)\n",
        "                grads = self.grad(self.deep_nn(features), features, labels)\n",
        "                optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables))\n",
        "                if step % verbose == 0:\n",
        "                    print(\"Iter: {}, Loss: {:.4f}\".format(step, self.loss_fn(self.deep_nn(features),features,labels)))\n",
        "\n",
        "    def test_model(self,x_data, y_data):\n",
        "        x_data, y_data = self.preprocess_data(x_data, y_data)\n",
        "        test_acc = self.accuracy_fn(self.deep_nn(x_data),y_data)\n",
        "        print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLG-Su1qXmuH"
      },
      "source": [
        "model = wide_deep_nn(nb_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9ToCjQ8XoH-",
        "outputId": "6b7e5db6-5957-4dbf-95cc-3e30188e9d16"
      },
      "source": [
        "model.fit(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.7717\n",
            "Iter: 500, Loss: 0.6877\n",
            "Iter: 1000, Loss: 0.6868\n",
            "Iter: 1500, Loss: 0.6859\n",
            "Iter: 2000, Loss: 0.6848\n",
            "Iter: 2500, Loss: 0.6838\n",
            "Iter: 3000, Loss: 0.6826\n",
            "Iter: 3500, Loss: 0.6813\n",
            "Iter: 4000, Loss: 0.6798\n",
            "Iter: 4500, Loss: 0.6783\n",
            "Iter: 5000, Loss: 0.6765\n",
            "Iter: 5500, Loss: 0.6744\n",
            "Iter: 6000, Loss: 0.6721\n",
            "Iter: 6500, Loss: 0.6694\n",
            "Iter: 7000, Loss: 0.6662\n",
            "Iter: 7500, Loss: 0.6624\n",
            "Iter: 8000, Loss: 0.6580\n",
            "Iter: 8500, Loss: 0.6526\n",
            "Iter: 9000, Loss: 0.6460\n",
            "Iter: 9500, Loss: 0.6379\n",
            "Iter: 10000, Loss: 0.6280\n",
            "Iter: 10500, Loss: 0.6156\n",
            "Iter: 11000, Loss: 0.6004\n",
            "Iter: 11500, Loss: 0.5814\n",
            "Iter: 12000, Loss: 0.5582\n",
            "Iter: 12500, Loss: 0.5298\n",
            "Iter: 13000, Loss: 0.4954\n",
            "Iter: 13500, Loss: 0.4543\n",
            "Iter: 14000, Loss: 0.4065\n",
            "Iter: 14500, Loss: 0.3533\n",
            "Iter: 15000, Loss: 0.2978\n",
            "Iter: 15500, Loss: 0.2443\n",
            "Iter: 16000, Loss: 0.1970\n",
            "Iter: 16500, Loss: 0.1579\n",
            "Iter: 17000, Loss: 0.1271\n",
            "Iter: 17500, Loss: 0.1034\n",
            "Iter: 18000, Loss: 0.0853\n",
            "Iter: 18500, Loss: 0.0714\n",
            "Iter: 19000, Loss: 0.0606\n",
            "Iter: 19500, Loss: 0.0522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyndzsqeXq54",
        "outputId": "cdf23f1a-ad87-4ba3-a195-bc6783f2d173"
      },
      "source": [
        "model.test_model(x_data, y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testset Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHdk-_pUew1i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}